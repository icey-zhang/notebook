<img width="451" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/588571ce-ac90-4f06-9eb9-03703302dae1"># 介绍
Mamba 模型利用选择性 SSM，不仅实现了序列长度的线性可扩展性，而且在语言建模任务中提供了具有竞争力的性能。这种成功启发了后续视觉任务的应用，并提出了将 Mamba 集成到基础视觉模型的研究。**Vim[60]**采用类似**vit的体系结构**，**结合双向Mamba块代替传统的变压器块**。**VMamba[32]**引入了一种新的**2D选择性扫描技术来扫描水平和垂直方向的图像**，并构建了一个类似于**Swin Transformer[33]**的分层模型。我们的研究扩展了这些最初的探索，专注于优化视觉任务的 S6 适应，我们实现了改进的性能结果。

| 模型 | 类似于 | 特点 |
|-------|-------|-------|
| Vim | Vit | 结合双向Mamba块代替传统的变压器块 |
| Vamba | Swin Transformer | 2D选择性扫描技术来扫描水平和垂直方向的图像 |

# 方法
本节描述 LocalMamba 的核心组件，从旨在增强模型从图像中挖掘细粒度细节的能力的局部扫描机制开始。随后，我们介绍了扫描方向搜索算法，这是一种创新的方法，可以识别不同层的最佳扫描序列，从而确保了全局和局部视觉线索的和谐集成。本节的最后一部分说明了 LocalMamba 框架在简单的普通架构和复杂的层次结构中的部署，展示了它在不同设置中的通用性和有效性。

## 视觉表示的局部扫描
我们的方法采用选择性扫描机制 S6，在处理一维因果序列数据方面表现出卓越的性能。该机制对输入进行因果处理，有效地捕获扫描段内的重要信息，类似于理解顺序单词之间依赖关系的语言建模。**然而，图像中二维空间数据固有的非因果性质对这种因果处理方法提出了重大挑战**。传统的扁平化空间令牌的策略损害了局部2D依赖关系的完整性，从而降低了模型有效识别空间关系的能力。例如，如图 1 (a) 和 (b) 所示，**Vim [60] 中使用的扁平化方法破坏了这些局部依赖关系，显着提高了垂直相邻标记之间的距离，并阻碍了模型捕获局部细微差别的能力**。虽然**VMamba[32]试图通过在水平和垂直方向上扫描图像来解决这个问题**，但它仍然无法在单个扫描中全面处理空间区域。

<img width="451" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/9c7c7715-458f-40ae-b4d4-bea1592dcf7f">

为了解决这一限制，我们引入了一种新的局部扫描图像的方法。通过将图像划分为多个不同的局部窗口，我们的方法确保了相关局部标记的更紧密排列，增强了局部依赖的捕获。这种技术如图 1 (c) 所示，将我们的方法与无法保持空间连贯性的先前方法进行了比较。**虽然我们的方法擅长在每个区域内有效地捕获局部依赖关系，但它也承认全局上下文的重要性**。为此，我们通过跨四个方向集成选择性扫描机制来构建我们的基本块：原始 (a) 和 (c) 方向，以及它们的翻转对应物，便于从尾部扫描到头部（Vim 和 VMamba 都采用翻转方向来更好地建模不寻常的图像标记）。这种多方面的方法确保了每个选择性扫描块内的综合分析，在局部细节和全局视角之间取得平衡。

<img width="451" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/dd32022a-50fd-4080-b42e-4be0d8a4b155">

如图3所示，我们的块通过四个不同的选择性扫描分支处理每个输入图像特征。这些分支独立捕获相关信息，随后合并为统一的特征输出。为了增强不同特征的集成并消除无关信息，我们在合并之前引入了一个空间和通道注意模块。如图 3b 所示，该模块自适应地对每个分支特征内的通道和标记进行加权，包括两个关键组件：通道注意分支和空间注意分支。**通道注意分支通过对空间维度上的输入特征进行平均来聚合全局表示，然后应用线性变换来确定通道权重。相反，空间注意机制通过使用全局表示增强每个令牌的特征来评估令牌重要性，从而实现微妙、重要性加权的特征提取。**

 <img width="457" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/6c24f225-7306-4ad5-88a8-fb84e5799fa4">

备注。虽然一些ViT变体，如Swin Transformer[33]，提出将图像分割为更小的窗口，但LocalMamba在目的和效果上都是不同的。**ViT中的加窗自我注意主要解决了全局自我注意的计算效率，尽管牺牲了一些全局注意能力。相反，我们的局部扫描机制旨在重新排列标记位置以增强视觉 Mamba 中局部区域依赖关系的建模，而全局理解能力被保留，因为整个图像仍然由 SSM 聚合和处理。**

## 搜索自适应扫描
结构化状态空间模型(SSM)在捕获图像表示方面的有效性在不同的扫描方向上是不同的。直观地实现最佳性能表明在各个方向上使用多个扫描，类似于我们之前讨论的 4 分支局部选择性扫描块。然而，这种方法大大增加了计算需求。为了解决这个问题，我们引入了一种策略来有效地为每一层选择最合适的扫描方向，从而在不产生过多的计算成本的情况下优化性能。该方法涉及搜索每一层的最佳扫描配置，确保定制高效的表示建模。

**搜索空间**。为了定制每一层的扫描过程，我们引入了 8 个候选扫描方向的不同集合 S。这些包括**水平和垂直扫描（标准和翻转）**，以及**窗口大小为 2 和 7 的局部扫描**（也是标准和翻转）。对于与之前的模型一致的计算预算，我们为每一层选择这 8 个方向中的 4 个。这种方法导致 $(C^4_8 )^K$ 的大量搜索空间，K 表示块的总数。基于DARTS[29]的原理，我们的方法对扫描方向应用了可微搜索机制，采用连续松弛来导航分类选择。这种方法转换离散选择过程进入一个连续的域，允许使用 softmax 概率来表示扫描方向的选择：

<img width="235" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/30e64245-2732-4642-9de5-8f57eacdab07">

其中 α(l) 表示每一层 l 的一组可学习参数，反映了所有潜在扫描方向的 softmax 概率。我们将整个搜索空间构建为一个过度参数化的网络，允许我们按照标准训练协议同时优化网络参数和架构变量 α。在完成训练后，我们通过选择具有最高 softmax 概率的四个方向来导出最佳方向选项。我们在图 4 中可视化了我们模型的搜索方向。有关搜索结果的详细分析，请参阅第 5.5 节。

<img width="457" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/7dc59c41-c1ee-40aa-89f6-d8b613d38b3a">


**方向搜索的可扩展性**。我们当前的方法聚合了训练中选择的所有扫描方向，这是一种具有中等范围的选项的模型。例如，每个块有 20 个块和 128 个方向的模型需要 28 GB 的 GPU 内存，这表明广泛选择的可扩展性限制。为了减轻具有大量选择阵列的场景中的内存消耗，单路径采样[15,57]、二进制近似[1]和部分通道使用[55]等技术提出了可行的解决方案。**我们将对更自适应的方向策略和高级搜索技术的调查留给未来的工作。**

# 结构变体
为了彻底评估我们方法的有效性，我们引入了基于普通 [60] 和分层 [32] 结构的架构变体，分别称为 LocalVim 和 LocalVMamba。这些体系结构的配置详见表1。具体来说，在LocalVim中，标准SSM块被替换为我们的LocalVim块，如图3所示。**考虑到原始的Vim块包括两个扫描方向(水平和翻转水平)**，我们的LocalVim引入了四个扫描方向，从而增加了计算开销。为了保持类似的计算预算，我们将 Vim 块的数量从 24 调整到 20。对于 LocalVMamba，它本质上有四个类似于我们模型的扫描方向，我们直接替换块而不更改结构配置。计算成本分析。我们的 LocalMamba 块高效且有效，计算成本仅略有增加。扫描机制只涉及重新定位令牌，在FLOPs方面不会产生额外的计算成本。此外，专为跨扫描有效地聚合不同信息而设计的 SCAttn 模块非常精简。它利用线性层将令牌维度减少 1/r 倍，然后在空间和通道维度上生成注意力权重，所有模型的 r 设置为 8。例如，我们的 LocalVMamba-T 模型，它将 VMamba 块替换为我们的 LocalMamba 块，仅将 VMamva-T 的 FLOPs 从 5.6G 增加到 5.7G。
