# YOLO-World: Real-Time Open-Vocabulary Object Detection

<img width="983" alt="image" src="https://github.com/icey-zhang/notebook/assets/54712081/0382fa34-c3d2-4732-99b8-d50697215c6b">

[【Paper】](https://arxiv.org/abs/2401.17270) [【Code】](https://github.com/AILab-CVC/YOLO-World)

## 摘要
You Only Look Once (YOLO)系列检测器已将自己确立为高效实用的工具。然而，它们对预定义和经过训练的对象类别的依赖限制了它们在开放场景中的适用性。为了解决这一限制，我们引入了 YOLO-World，这是一种创新的方法，通过视觉语言建模和大规模数据集的预训练来增强 YOLO，具有开放词汇检测能力。具体来说，我们提出了一种新的可重新参数化视觉语言路径聚合网络（RepVL-PAN）和区域-文本对比损失，以促进视觉和语言信息之间的交互。我们的方法擅长以高效率的零样本方式检测广泛的对象。在具有挑战性的LVIS数据集上，YOLO-World在V100上实现了35.4 AP，在精度和速度方面都优于许多最先进的方法。此外，微调后的 YOLO-World 在几个下游任务（包括对象检测和开放词汇实例分割）上取得了显着的性能。

## 介绍
目标检测是计算机视觉中一个长期而根本的挑战，在图像理解、机器人和自动驾驶汽车等领域有着广泛的应用。大量的工作[16,27,43,45]随着深度神经网络的发展，目标检测取得了重大突破。尽管这些方法取得了成功，但它们仍然有限，因为它们只处理具有固定词汇表的对象检测，例如 COCO [26] 数据集中的 80 个类别。一旦定义了对象类别并标记，经过训练的检测器只能检测这些特定的类别，从而限制了开放场景的能力和适用性。最近的工作[8,13,48,53,58]探索了流行的视觉语言模型[19,39]，通过从语言编码器(如BERT[5])中提取词汇知识来解决开放词汇检测[58]。然而，由于词汇量有限的训练数据稀缺，这些基于蒸馏的方法非常有限，例如包含 48 个基本类别的 OV-COCO [58]。几种方法[24,30,56,57,59]将目标检测训练重新定义为**区域级视觉语言预训练**，并大规模训练开放词汇对象检测器。然而，这些方法在现实场景中仍然难以检测，**这有两个方面：（1）沉重的计算负担和（2）边缘设备的复杂部署。**

先前的工作已经展示了**预训练大型检测器**的良好性能，同时**预训练小型检测器**以**赋予它们开放识别能力**仍未得到探索。

在本文中，我们提出了 YOLO-World，旨在高效开放词汇对象检测，并探索大规模预训练方案，将传统的 YOLO 检测器提升到一个新的开放词汇世界。与以前的方法相比，所提出的 YOLOWorld 在推理速度高且易于部署到下游应用程序中非常有效。具体来说，**YOLO-World 遵循标准的 YOLO 架构 [20]，并利用预训练的 CLIP [39] 文本编码器对输入文本进行编码**。我们进一步提出了可**重新参数化的视觉语言路径聚合网络（RepVL-PAN）来连接文本特征和图像特征以获得更好的视觉语义表示**。在推理过程中，可以删除文本编码器，并将文本嵌入重新参数化为 RepVL-PAN 的权重以实现高效部署。我们通过大规模数据集上的区域-文本对比学习进一步研究了YOLO检测器的开放词汇预训练方案，将检测数据、接地数据和图像-文本数据统一为区域-文本对。具有丰富区域-文本对的预训练 YOLO-World 在大规模词汇检测和训练更多数据方面表现出强大的能力，可以显着提高开放词汇能力。

此外，我们探索了一种提示然后检测范式，以进一步提高现实场景中开放词汇对象检测的效率。如图 2 所示，传统的目标检测器 [16, 20, 23, 41-43, 52] 专注于具有预定义和训练类别的固定词汇表（闭集）检测。虽然以前的开放词汇检测器[24,30,56,59]使用文本编码器对用户的提示进行编码，以检测对象。值得注意的是，这些方法倾向于使用具有大量主干的大型检测器，例如 Swin-L [32]，以增加开放词汇容量。相比之下，prompt-thendetect 范式（图 2 (c)）首先对用户的提示进行编码以构建离线词汇表，词汇表因不同需求而异。然后，高效的检测器可以动态推断离线词汇表，而无需重新编码提示。对于实际应用，一旦我们训练了检测器，即 YOLO-World，我们可以预先编码提示或类别以构建离线词汇表，然后将其无缝集成到检测器中。

我们的主要贡献可以概括为三个方面： 

• 我们介绍了 YOLO-World，这是一种尖端开放词汇对象检测器，在实际应用中具有高效率。

• 我们提出了一种可重新参数化的视觉语言PAN来连接视觉和语言特征和YOLO-World的开放词汇区域-文本对比预训练方案。

• 在大规模数据集上预训练的 YOLO-World 表现出强大的零样本性能，并在 LVIS 上以 52.0 FPS 实现了 35.4 AP。预训练的 YOLO-World 可以轻松适应下游任务，例如开放词汇实例分割和参考对象检测。此外，YOLO-World 的预训练权重和代码将被开源，以促进更实用的应用。


## 相关工作
### 开放词汇对象检测 (OVD)
开放词汇对象检测 (OVD) [58] 已成为现代目标检测的新趋势，旨在检测预定义类别之外的对象。早期的工作[13]通过在基类上训练检测器并评估新的(未知)类，遵循标准的OVD设置[58]。然而，这种开放词汇设置可以评估检测器检测和识别新对象的能力，对于开放场景仍然受到限制，并且由于在有限的数据集和词汇上进行训练，缺乏对其他领域的泛化能力。 [58] 已成为现代目标检测的新趋势，旨在检测预定义类别之外的对象。早期的工作[13]通过在基类上训练检测器并评估新的(未知)类，遵循标准的OVD设置[58]。然而，这种开放词汇设置可以评估检测器检测和识别新对象的能力，对于开放场景仍然受到限制，并且由于在有限的数据集和词汇上进行训练，缺乏对其他领域的泛化能力。

受视觉语言预训练[19,39]启发，最近的工作[8,22,53,62,63]将开放词汇对象检测表述为图像-文本匹配，并利用大规模图像-文本数据大规模增加训练词汇。**OWLViT** [35, 36] 使用检测和接地数据集微调简单的视觉转换器 [7]，并构建具有有希望的性能的简单开放词汇检测器。**GLIP** [24] 提出了一种基于短语接地的开放词汇检测框架，并在零样本设置中进行评估。Grounding DINO[30]将接地的预训练[24]合并到具有跨模态融合的检测变压器[60]中。几种方法[25,56,57,59]通过区域-文本匹配和大规模图像-文本对的预训练检测器统一检测数据集和图像-文本数据集，取得了良好的性能和泛化能力。然而，这**些方法通常使用像ATSS[61]或DINO[60]这样的重检测器，Swin-L[32]作为骨干，导致计算需求高和部署挑战**。相比之下，我们提出了 YOLO-World，旨在通过实时推理和更容易的下游应用程序部署进行有效的开放词汇对象检测。与 <font color=Blue>**ZSD-YOLO** [54]</font>不同，ZSD-YOLO [54] 还通过语言模型对齐探索了带有 YOLO 的开放词汇检测 [58]，YOLO-World 引入了一种新颖的 YOLO 框架，具有**有效的预训练策略**，增强了开放词汇性能和泛化。

## 方法
### 方法部分详细翻译

#### 3.1 预训练公式：区域-文本对

传统的目标检测方法，包括 YOLO 系列 [20]，是用实例注释 \(\Omega = \{B_i, c_i\}_{i=1}^N\) 进行训练的，其中包括边界框 \(\{B_i\}\) 和类别标签 \(\{c_i\}\)。本文将实例注释重新定义为区域-文本对 \(\Omega = \{B_i, t_i\}_{i=1}^N\)，其中 \(t_i\) 是对应于区域 \(B_i\) 的文本。具体来说，文本 \(t_i\) 可以是类别名称、名词短语或物体描述。此外，YOLO-World 采用图像 \(I\) 和文本 \(T\)（一组名词）作为输入，并输出预测框 \(\{B_k\}\) 及相应的物体嵌入 \(\{e_k\}\)（\(e_k \in \mathbb{R}^D\)）。

#### 3.2 模型架构

本文提出的 YOLO-World 的总体架构如图 3 所示，包括一个 YOLO 检测器、一个文本编码器和一个可重新参数化的视觉-语言路径聚合网络（RepVL-PAN）。给定输入文本，YOLO-World 中的文本编码器将文本编码成文本嵌入。YOLO 检测器中的图像编码器从输入图像中提取多尺度特征。然后我们利用 RepVL-PAN 通过图像特征和文本嵌入之间的跨模态融合来增强文本和图像的表示。

##### YOLO 检测器

YOLO-World 主要基于 YOLOv8 [20] 开发，包含一个 Darknet 骨干网络 [20, 43] 作为图像编码器，一个路径聚合网络（PAN）用于多尺度特征金字塔，以及一个用于边界框回归和物体嵌入的头部。

##### 文本编码器

给定文本 \(T\)，我们采用由 CLIP [39] 预训练的 Transformer 文本编码器来提取相应的文本嵌入 \(W = \text{TextEncoder}(T) \in \mathbb{R}^{C \times D}\)，其中 \(C\) 是名词的数量，\(D\) 是嵌入维度。与仅文本的语言编码器 [5] 相比，CLIP 文本编码器在连接视觉对象和文本方面具有更好的视觉语义能力。当输入文本是标题或引用表达时，我们采用简单的 n-gram 算法来提取名词短语，然后将其输入到文本编码器中。

##### 文本对比头

跟随之前的工作 [20]，我们采用解耦头部，通过两个 3×3 的卷积来回归边界框 \(\{b_k\}_{k=1}^K\) 和物体嵌入 \(\{e_k\}_{k=1}^K\)，其中 \(K\) 表示物体的数量。我们提出一个文本对比头，以获得物体-文本相似性 \(s_{k,j}\)，公式如下：

\[ s_{k,j} = \alpha \cdot \text{L2-Norm}(e_k) \cdot \text{L2-Norm}(w_j)^\top + \beta, \]

其中 \(\text{L2-Norm}(\cdot)\) 是 L2 归一化，\(w_j \in W\) 是第 \(j\) 个文本嵌入。此外，我们添加了具有可学习缩放因子 \(\alpha\) 和偏移因子 \(\beta\) 的仿射变换。L2 归一化和仿射变换对于稳定区域-文本训练非常重要。

##### 在线词汇训练

在训练过程中，我们为每个包含 4 张图像的马赛克样本构建一个在线词汇表 \(T\)。具体来说，我们对马赛克图像中涉及的所有正向名词进行采样，并从相应的数据集中随机采样一些负向名词。每个马赛克样本的词汇最多包含 \(M\) 个名词，默认设置 \(M\) 为 80。

##### 离线词汇推理

在推理阶段，我们提出了一种带有离线词汇的提示-然后-检测策略，以进一步提高效率。如图 3 所示，用户可以定义一系列自定义提示，包括标题或类别。然后我们利用文本编码器对这些提示进行编码，获得离线词汇嵌入。离线词汇避免了每次输入的计算，并提供了根据需要调整词汇的灵活性。

#### 3.3 可重新参数化的视觉-语言路径聚合网络

图 4 显示了提出的 RepVL-PAN 的结构，该结构遵循 [20, 29] 中的自顶向下和自底向上路径，建立多尺度图像特征金字塔 \(\{P_3, P_4, P_5\}\)。此外，我们提出了文本引导的 CSPLayer（T-CSPLayer）和图像池化注意力（I-Pooling Attention），以进一步增强图像特征和文本特征之间的交互，这可以提高开放词汇能力的视觉语义表示。在推理过程中，离线词汇嵌入可以重新参数化为卷积层或线性层的权重，以进行部署。

##### 文本引导的 CSPLayer

如图 4 所示，跨阶段部分层（CSPLayer）在自顶向下或自底向上融合后被利用。我们通过将文本指导纳入多尺度图像特征，扩展了 [20] 的 CSPLayer（也称为 C2f），形成文本引导的 CSPLayer。具体来说，给定文本嵌入 \(W\) 和图像特征 \(X_l \in \mathbb{R}^{H \times W \times D}\)（\(l \in \{3, 4, 5\}\），我们在最后一个黑暗瓶颈块之后采用最大-sigmoid 注意力，将文本特征聚合到图像特征中，公式如下：

\[ X'_l = X_l \cdot \delta \left( \max_{j \in \{1..C\}} (X_l W_j^\top) \right)^\top, \]

其中更新后的 \(X'_l\) 与跨阶段特征连接作为输出。\(δ\) 表示 sigmoid 函数。

##### 图像池化注意力

为了增强具有图像感知信息的文本嵌入，我们通过提出图像池化注意力来聚合图像特征以更新文本嵌入。与直接使用图像特征上的交叉注意力不同，我们在多尺度特征上采用最大池化来获得 3×3 区域，结果为 27 个补丁令牌 \(X̃ \in \mathbb{R}^{27 \times D}\)。然后我们通过以下公式更新文本嵌入：

\[ W' = W + \text{MultiHead-Attention}(W, X̃, X̃). \]

#### 3.4 预训练方案

本节介绍了在大规模检测、定位和图像-文本数据集上预训练 YOLO-World 的训练方案。

##### 使用区域-文本对比损失进行学习

给定马赛克样本 \(I\) 和文本 \(T\)，YOLO-World 输出 \(K\) 个物体预测 \(\{B_k, s_k\}_{k=1}^K\) 以及注释 \(\Omega = \{B_i, t_i\}_i^N\)。我们遵循 [20] 的方法，利用任务对齐标签分配 [9] 将预测与真实注释匹配，并将每个正向预测分配一个文本索引作为分类标签。基于此词汇，我们通过交叉熵在物体-文本（区域-文本）相似性和物体-文本分配之间构建区域-文本对比损失 \(L_{\text{con}}\)。此外，我们采用 IoU 损失和分布式焦点损失进行边界框回归，整体训练损失定义如下：

\[ L(I) = L_{\text{con}} + \lambda_I \cdot (L_{\text{iou}} + L_{\text{dfl}}), \]

其中 \(λ_I\) 是一个指示因子，当输入图像 \(I\) 来自检测或定位数据时，设置为 1；当输入图像来自图像-文本数据时，设置为 0。考虑到图像-文本数据集包含噪声框，我们只对具有准确边界框的样本计算回归损失。

##### 使用图像-文本数据进行伪标签生成

与直接使用图像-文本对进行预训练不同，我们提出了一种自动标注方法来生成区域-文本对。具体来说，该标注方法包含三个步骤：






