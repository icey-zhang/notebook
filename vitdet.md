ViTDet是Meta AI团队（kaiming团队）在MAE之后提出的基于原生ViT模型作为骨干网络的检测模型。在最早的论文Benchmarking Detection Transfer Learning with Vision Transformers中，作者初步研究了以ViT作为骨干网络的检测模型所面临的挑战（架构的不兼容，训练速度慢以及显存占用大等问题），并给出了具体的解决方案，最重要的是发现基于MAE的预训练模型展现了较强的下游任务迁移能力，效果大大超过随机初始化和有监督预训练模型。而最新的论文Exploring Plain Vision Transformer Backbones for Object Detection对上述工作做了进一步的拓展和优化，给出了性能更好的ViTDet，目前代码已经开源在detectron2的projects，这篇文章将主要结合第二篇论文和代码解读ViTDet。
